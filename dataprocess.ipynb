{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Columns: 354 entries, 0 to 353\n",
      "dtypes: object(354)\n",
      "memory usage: 110.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Columns: 354 entries, 0 to 353\n",
      "dtypes: object(354)\n",
      "memory usage: 110.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "old_data = pd.read_excel(\"data_285_313.xlsx\",sheet_name=\"操作变量\",header=0) # 读取325个样本的数据，不读取表头\n",
    "old_data.drop('样本285原始数据',axis= 1,inplace=True)  ##将第一列删除\n",
    "operation_var = old_data.iloc[1,:]\n",
    "operation_var = operation_var.to_list()  #获得操作变量的信息\n",
    "print(len(operation_var))\n",
    "old_data.drop(0,axis= 0,inplace=True)\n",
    "old_data.drop(1,axis= 0,inplace=True)\n",
    "old_data.drop(42,axis=0,inplace=True)\n",
    "\n",
    "# print(old_data.shape)\n",
    "\n",
    "old_data = old_data.values\n",
    "data_285 = old_data[0:40,:]  ## 获得第285号样本的数据\n",
    "data_313 = old_data[40:80,:] ## 获得第313号样本的数据\n",
    "\n",
    "df_285 = pd.DataFrame(data_285)\n",
    "df_313 = pd.DataFrame(data_313)\n",
    "## 判断有无缺失值\n",
    "df_285.info()\n",
    "df_313.info() #均没有缺失值\n",
    "# print(data_285.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 下面根据附件二的要求先对285号样本和313号样本进行数据预处理--第一步\n",
    "\n",
    "# 首先遍历每一列，找到每一列中0占比较多的列索引，方便问题二建模同，这里将0比例为0.5设为阈值\n",
    "def find_zero_column(data):\n",
    "    zero_lst = []\n",
    "    for i in range(data.shape[1]):\n",
    "        num = 0\n",
    "        for j in range(data.shape[0]):\n",
    "            if data[j,i]==0:\n",
    "                num = num+1\n",
    "        if num/data.shape[0]>=0.5:\n",
    "            zero_lst.append(i)\n",
    "    return zero_lst\n",
    "lst1 = find_zero_column(data_285)\n",
    "lst2 = find_zero_column(data_313)\n",
    "\n",
    "\n",
    "del_var = list(set(lst1+lst2))  ## 第一步预处理中剔除掉的操作变量，也是问题二中进行模型预测之前需要剔除掉的操作变量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 第二步根据附件四的信息读取每一个操作变量的所允许的最大值以及最小值\n",
    "var_constr = pd.read_excel(\"附件四：354个操作变量信息.xlsx\",header=0) \n",
    "var_constr.head()\n",
    "\n",
    "## 读取取值范围所在的列\n",
    "var_names = var_constr['中文名称']\n",
    "var_constr = var_constr['取值范围']\n",
    "var_constrs = var_constr.to_list()  ##获取每个操作变量的取值范围，但此时列表里的元素还不是我们想要的\n",
    "# print(var_constrs)\n",
    "\n",
    "## 接下来对上面获得var_constrs 进行切片操作\n",
    "demo = var_constrs[0]\n",
    "d = demo.split('-')\n",
    "var_min = {}  ##存储操作变量最小值的字典\n",
    "var_max = {}  ##存储操作变量最大值的字典\n",
    "## 下面对 var_constrs中的元素进行拆分\n",
    "for i in range(len(var_constrs)): ##遍历var_constrs中的每一个元素\n",
    "    var = var_constrs[i]\n",
    "    temp = var.split('-',1) # 设置最大分割次数为1，分隔符为-，注意：有些变量最小值是负数，因此要分类讨论\n",
    "    # print(temp)\n",
    "    if temp[0] != '':  ## 如果分割之后的第一个元素不是''说明该数据不是负数，可以被正确存储到var_min中\n",
    "        var_min[i] = float(temp[0])\n",
    "        var_max[i] = float(temp[1]) \n",
    "    else:\n",
    "        ## 如果temp[0] =''就意味着最小值是负数\n",
    "        # 继续分割temp[1]\n",
    "        var1 = temp[1].split('-',1)\n",
    "        var_min[i] = -float(var1[0])  ##分割第二次后的第一个元素就是最小值的负数，可以直接添加\n",
    "        ## 但是对于第二次分割后的第二个元素还要继续细分，如果遇到了括号（），就不能直接添加，所以下面对字符串进行去括号操作\n",
    "        var2 = var1[1].replace('(','').replace(')','')\n",
    "        var_max[i] = float(var2)\n",
    "    \n",
    "var_min_lst = [j  for i,j in var_min.items()]  ## 354个操作变量的最小取值\n",
    "var_max_lst = [j  for i,j in var_max.items()]  ## 354个操作变量的最大取值\n",
    "\n",
    "# var_names.to_csv('操作变量取值.csv')\n",
    "# new_df = pd.read_csv('操作变量取值.csv')\n",
    "# new_df['最小值'] = var_min_lst\n",
    "# new_df['最大值'] = var_max_lst\n",
    "# new_df.to_csv('操作变量取值.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 下面对第285号样本以及313号样本进行第二步预处理：筛选操作变量值不在 var_min_lst 以及var_max_lst之间的样本数据重置为0\n",
    "def second_pre(data):\n",
    "    minmax_array = np.zeros((data.shape[0],data.shape[1]))  ##初始化一个新的数组\n",
    "    for i in range(data.shape[1]):  ##遍历每一个操作变量\n",
    "        for j in range(data.shape[0]): ##遍历每一个时间点\n",
    "            if (data[j][i] <var_min_lst[i] or data[j][i]>var_max_lst[i]):  ##如果data中的数据不在正常范围内\n",
    "                minmax_array[j][i] = 0 ##初始该位置的数据为0，方便后续使用\n",
    "            else:\n",
    "                minmax_array[j][i] = data[j][i] ##否则不变\n",
    "    return minmax_array\n",
    "\n",
    "## 至此，异常值筛选完毕\n",
    "new_285 = second_pre(data_285)\n",
    "new_313 = second_pre(data_313)\n",
    "# print(new_285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 根据拉伊达法则对数据进行第三步预处理\n",
    "def layida(data):\n",
    "    ## 默认数据的列向量是等精度测量的数据\n",
    "    mu_data = np.sum(data,axis=0)/data.shape[0]  ##获得数组的平均值\n",
    "    sigma_data = np.sqrt(np.sum(data**2,axis=0)/(data.shape[0]-1)) ##获得数组的方差值\n",
    "    new_data = np.zeros((data.shape[0],data.shape[1])) ##初始化新的数组矩阵\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if abs(data[i][j]-mu_data[j])>3*sigma_data[j]: ##如果满足拉伊达法则，就把原数据赋值为0，否则不变\n",
    "                new_data[i][j] =0\n",
    "            else:\n",
    "                new_data[i][j] = data[i][j] \n",
    "    return new_data\n",
    "\n",
    "new1_285 = layida(new_285) \n",
    "new1_313 = layida(new_313)\n",
    "# print(new1_285)\n",
    "# print(new1_313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[18, 19, 25, 27, 37]\n",
      "(40, 354)\n",
      "(35, 354)\n"
     ]
    }
   ],
   "source": [
    "## 统计每一行数据为0的个数，如果该时间点中的数据值为0的个数20，那么将该行数据删除\n",
    "def count_zero(data,limit):\n",
    "    zero_num = []\n",
    "    for i in range(data.shape[0]):\n",
    "        # zero_num.append(np.sum(np.where(data[i,:]==0,1,0))) ##统计每一行元素为0的个数\n",
    "        temp = np.sum(np.where(data[i,:]==0,1,0))\n",
    "        if temp>=limit:\n",
    "            zero_num.append(i)\n",
    "    print(zero_num)\n",
    "    new_data = np.delete(data,zero_num,axis=0)\n",
    "    return new_data\n",
    "\n",
    "new2_285 = count_zero(new1_285,20)\n",
    "new2_313 = count_zero(new1_313,20)\n",
    "print(new2_285.shape)\n",
    "print(new2_313.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 对第285号、313号数据进行最后的处理\n",
    "last_285 = np.sum(new2_285,axis=0)/new2_285.shape[0]\n",
    "last_313 = np.sum(new2_313,axis=0)/new2_313.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 将最终处理的数据last_285,last_313添加入data_325.xlsx中\n",
    "data_325 = pd.read_excel(\"data_325.xlsx\",header=0)\n",
    "data = data_325.values\n",
    "data[284,14:] = last_285  # 将整理好的第285号样本数据整理到原数据集中\n",
    "data[312,14:] = last_313  # 将整理好的第313号样本数据整理到原数据集中\n",
    "# print(data[0,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 87, 91, 102, 218, 221, 229, 312]\n",
      "(325, 359)\n"
     ]
    }
   ],
   "source": [
    "## 现在对整体数据进行预处理\n",
    "end_zero_col = find_zero_column(data)  # 将缺失数据较多的操作单位所在的列删除\n",
    "print(end_zero_col)\n",
    "## 删除\n",
    "end_data = np.delete(data,end_zero_col,axis=1)   ## 删除不需要的列\n",
    "P2_data = np.delete(end_data,9,axis=1)\n",
    "print(P2_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_arr = P2_data[:,7:9]\n",
    "# print(y_arr)\n",
    "x_arr = np.delete(P2_data,[7,8],axis=1)\n",
    "# print(x_arr.shape)\n",
    "\n",
    "p2_data = np.hstack([x_arr,y_arr]) ##最右边两列是产品性质变量，做预测用，前面的是因变量\n",
    "import scipy.io as io\n",
    "io.savemat('p2.mat',{'p2':p2_data})\n",
    "raw_mater_Var = ['原料硫含量','原料辛烷值','原料饱和烃','原料烯烃','原料芳烃','原料溴值','原料密度','待生吸附-焦碳','待生吸附S','再生吸附-焦炭','再生吸附']\n",
    "var_name = raw_mater_Var+['产品硫含量','产品辛烷值','产品辛烷值损失'] + operation_var\n",
    "# print(len(var_name))\n",
    "var_names = [n for i ,n in enumerate(var_name) if i not in end_zero_col]\n",
    "var_names.remove('产品辛烷值损失')\n",
    "var_names.remove('产品辛烷值')\n",
    "var_names.remove('产品硫含量')\n",
    "Var_names= var_names+ ['产品硫含量','产品辛烷值']\n",
    "data = Var_names\n",
    "filename = 'columns.txt'  # 保存的文件名\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    for item in data:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
